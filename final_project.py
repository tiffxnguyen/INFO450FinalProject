# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/111wKN1Enp4gwyXWn-hsL0bGOTeImdXXF

Part 1
"""

# Download and load data
import urllib.request, os, time

url = "https://storage.googleapis.com/info_450/IndividualAssistanceHousingRegistrantsLargeDisasters%20(1).csv"
filename = "fema_disaster_data.csv"

start = time.time()
print(f"Downloading {filename}...")
urllib.request.urlretrieve(url, filename)

import pandas as pd
df = pd.read_csv(filename, nrows=300000)
df.info()
df.head()

# Handle missing values
import numpy as np

df_clean = df.copy()

df_clean.columns = df_clean.columns.str.strip()

# Used AI for this section
def standardize_binary_col(df, col_name, replace_map=None):
    if col_name in df.columns:
        if replace_map:
            df[col_name] = df[col_name].replace(replace_map)
        df[col_name] = pd.to_numeric(df[col_name], errors='coerce').astype('Int64')
    return df

for col in ['repairAmount', 'grossIncome', 'waterLevel']:
    if col in df_clean.columns:
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

df_clean = standardize_binary_col(df_clean, 'tsaEligible')

df_clean = standardize_binary_col(df_clean, 'destroyed', {'yes':1, 'Yes':1, 'Y':1, 'No':0, 'no':0, 'N':0})

relevant = ['tsaEligible', 'repairAmount', 'grossIncome', 'residenceType', 'damagedStateAbbreviation']
missing_summary = df_clean[relevant].isna().sum()
print("Missing values summary after initial cleaning:")
display(missing_summary)

# Crosstab
ct_residence = pd.crosstab(df_clean['residenceType'], df_clean['tsaEligible'], normalize='index').fillna(0)
ct_state = pd.crosstab(df_clean['damagedStateAbbreviation'], df_clean['tsaEligible'], normalize='index').fillna(0)

ct_residence.head(), ct_state.head()

# Groupby
avg_repair_by_state = df_clean.dropna(subset=['repairAmount','damagedStateAbbreviation']).groupby('damagedStateAbbreviation')['repairAmount'].mean().sort_values(ascending=False)
avg_repair_by_state.head(20)

# Charts
import plotly.express as px

# Bar chart: TSA eligibility rate by state
state_rates = pd.crosstab(df_clean['damagedStateAbbreviation'], df_clean['tsaEligible'], normalize='index').fillna(0)
state_rates = state_rates.reset_index().rename(columns={0:'not_eligible', 1:'eligible'})
fig_bar = px.bar(state_rates.sort_values('eligible', ascending=False).head(30),
                 x='damagedStateAbbreviation', y='eligible',
                 title='Top 30 States by TSA Eligibility Rate', labels={'eligible':'TSA eligible rate'})

# Histogram: distribution of repairAmount
fig_hist = px.histogram(df_clean.dropna(subset=['repairAmount']), x='repairAmount', nbins=50, title='Distribution of repairAmount')
fig_hist.update_xaxes(type='log')

# Boxplot: repairAmount across residence types
fig_box_res = px.box(df_clean.dropna(subset=['repairAmount','residenceType']),
                     x='residenceType', y='repairAmount', title='Repair Amount by Residence Type')

# Analyst choice
df_bin = df_clean.dropna(subset=['grossIncome'])
df_bin['income_bin'] = pd.qcut(df_bin['grossIncome'].rank(method='first'), q=5, labels=['Q1','Q2','Q3','Q4','Q5'])
income_rates = pd.crosstab(df_bin['income_bin'], df_bin['tsaEligible'], normalize='index').reset_index().rename(columns={1:'eligible'})
fig_income = px.bar(income_rates, x='income_bin', y='eligible', title='TSA Eligibility Rate by Income Quintile')

fig_bar.show()
fig_hist.show()
fig_box_res.show()
fig_income.show()

"""Interpretation of Charts

1.  Top States by TSA Eligibility Rate (Bar Chart):
    TSA States like Puerto Rico (PR) show a very high eligibility rate (around 92%), which means a significant portion of registrants in those areas qualified for TSA. States like South Carolina and North Carolina have very low eligibility rates (0% and 4.6%), which suggests that fewer registrants from these states met the criteria for TSA during the disasters.

2.  Distribution of Repair Amount (Histogram):
    The histogram for repairAmount shows a  right-skewed distribution. The x-axis has a long tail extending towards higher values, which confirms that most repair amounts are low. This means that while many registrants require smaller repair amounts, a few cases involve significantly higher costs. Most of the data is concentrated at the lower end of the repair cost spectrum.

3.  Repair Amount by Residence Type (Box Plot):
    The box plot shows the distribution of repairAmount across various residence types. There are differences in median repair amounts and the spread of values among different residence types. For example, House/Duplex shows a broader range of repair amounts and a higher median compared to Apartment or Condo, which might have more standardized repair needs. Mobile Home also show distinct repair cost patterns.

4.  TSA Eligibility Rate by Income Quintile (Bar Chart):
    This bar chart shows the TSA eligibility rate categorized by income quintiles (Q1 being the lowest income, Q5 the highest). As income quintile increases, the TSA eligibility rate decreases. For example, Q1 has the highest eligibility rate (around 49%), which progressively drops to Q5 (highest income) with the lowest eligibility rate (around 30%). This  suggests that lower-income individuals are more likely to be eligible for TSA.

Part 2
"""

# Inferential stats
from scipy import stats
import numpy as np

tsa_yes = df_clean[df_clean['tsaEligible'] == 1]['repairAmount']
tsa_no = df_clean[df_clean['tsaEligible'] == 0]['repairAmount']

def get_ci(series):
    mean = series.mean()
    sd = series.std()
    n = len(series)
    sem = sd / np.sqrt(n)
    ci = stats.t.interval(0.95, df=n-1, loc=mean, scale=sem)
    return mean, ci

mean_yes, ci_yes = get_ci(tsa_yes.dropna())
mean_no, ci_no = get_ci(tsa_no.dropna())

print("95% Confidence Intervals")
print(f"TSA Eligible Mean Repair Amount: ${mean_yes:.2f}")
print(f"95% CI: {ci_yes}")

print(f"\nNot TSA Eligible Mean Repair Amount: ${mean_no:.2f}")
print(f"95% CI: {ci_no}")

state1 = "FL"
state2 = "TX"

s1 = df_clean[df_clean['damagedStateAbbreviation'] == state1]['repairAmount']
s2 = df_clean[df_clean['damagedStateAbbreviation'] == state2]['repairAmount']

mean_s1, ci_s1 = get_ci(s1.dropna())
mean_s2, ci_s2 = get_ci(s2.dropna())

print("\nState Comparison")
print(f"{state1} Mean Repair Amount = ${mean_s1:.2f}, 95% CI = {ci_s1}")
print(f"{state2} Mean Repair Amount = ${mean_s2:.2f}, 95% CI = {ci_s2}")

# TSA t-test
t_tsa, p_tsa = stats.ttest_ind(tsa_yes.dropna(), tsa_no.dropna(), equal_var=False)

# State t-test
t_state, p_state = stats.ttest_ind(s1.dropna(), s2.dropna(), equal_var=False)

print("\nHypothesis Tests")

print("\nTSA Eligible vs Non-Eligible (Repair Amount)")
print(f"T-statistic = {t_tsa:.3f}, p-value = {p_tsa:.4f}")

print(f"\n{state1} vs {state2} (Repair Amount)")
print(f"T-statistic = {t_state:.3f}, p-value = {p_state:.4f}")

"""Interpretation of Inferential Statistics

1.  TSA Eligibility and Repair Amount Confidence Intervals:
    The 95% confidence interval for the mean repair amount for TSA-eligible individuals is approximately \[$5845.65, $6117.42\], with a mean of \$5981.53. The 95% confidence interval for the mean repair amount for individuals not eligible for TSA is approximately \[$4811.59, $5054.22\], with a mean of \$4932.90. The confidence intervals do not overlap, meaning there is a significant difference between the mean repair amounts of TSA-eligible and non-eligible individuals. TSA-eligible individuals tend to have a higher mean repair amount.

2.  State Comparison (FL vs. TX) and Repair Amount Confidence Intervals:
    The 95% confidence interval for the mean repair amount in Florida is approximately \[$3946.39, $4346.32\], with a mean of \$4146.36. The 95% confidence interval for the mean repair amount in Texas is approximately \[$8200.15, $8627.59\], with a mean of \$8413.87. The confidence intervals for Florida and Texas's mean repair amounts do not overlap. This means there is a significant difference with Texas having a higher mean repair amount compared to Florida.

3.  Hypothesis Tests (T-tests):
    The t-test of TSA Eligible and Non-Eligible resulted in a T-statistic of 11.284 and a p-value of 0.0000. Since the p-value is less than the typical significance level (0.05), we reject the null hypothesis. This provides evidence that there is a significant difference in the mean repair amounts between TSA-eligible and non-eligible individuals. The t-test of the two states resulted in a T-statistic of -28.587 and a p-value of 0.0000. This very low p-value means we reject the null hypothesis. There is evidence of a significant difference in the mean repair amounts between Florida and Texas.

Part 3
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

df_m = df_clean.copy()

cat_cols = ['residenceType', 'damagedStateAbbreviation']
enc = OrdinalEncoder()
df_m[cat_cols] = enc.fit_transform(df_m[cat_cols].astype(str))

X = df_m[['grossIncome','repairAmount','destroyed','waterLevel'] + cat_cols]
y = df_m['tsaEligible'].astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

# Model 1: Decision Tree
dt = DecisionTreeClassifier(max_depth=6, random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

# Model 2: Random Forest
rf = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Decision Tree Metrics:")
display({
    'accuracy': accuracy_score(y_test, y_pred_dt),
    'precision': precision_score(y_test, y_pred_dt, zero_division=0),
    'recall': recall_score(y_test, y_pred_dt, zero_division=0),
    'confusion_matrix': confusion_matrix(y_test, y_pred_dt).tolist()
})

print("\nRandom Forest Metrics:")
display({
    'accuracy': accuracy_score(y_test, y_pred_rf),
    'precision': precision_score(y_test, y_pred_rf, zero_division=0),
    'recall': recall_score(y_test, y_pred_rf, zero_division=0),
    'confusion_matrix': confusion_matrix(y_test, y_pred_rf).tolist()
})

print("\nRandom Forest Feature Importances:")
feat_importances = dict(zip(X.columns, rf.feature_importances_))
display(sorted(feat_importances.items(), key=lambda x: x[1], reverse=True))

"""Model Comparison and Generalization

Decision Tree Metrics:
*   Accuracy: 0.7250
*   Precision: 0.4439
*   Recall: 0.3542
*   Confusion Matrix: [[47291, 7461], [13009, 7239]]
    *   True Negatives (TN): 47291 (Correctly predicted not TSA eligible)
    *   False Positives (FP): 7461 (Incorrectly predicted TSA eligible)
    *   False Negatives (FN): 13009 (Incorrectly predicted not TSA eligible)
    *   True Positives (TP): 7239 (Correctly predicted TSA eligible)

Random Forest Metrics:
*   Accuracy: 0.7679
*   Precision: 0.6128
*   Recall: 0.3956
*   Confusion Matrix: [[50359, 4393], [12285, 7963]]
    *   True Negatives (TN): 50359
    *   False Positives (FP): 4393
    *   False Negatives (FN): 12285
    *   True Positives (TP): 7963

Random Forest Feature Importances:
1.  damagedStateAbbreviation: 0.795
2.  waterLevel: 0.098
3.  grossIncome: 0.049
4.  residenceType: 0.036
5.  repairAmount: 0.021
6.  destroyed: 0.001

Based on  metrics, the Random Forest Classifier generalizes better than the Decision Tree Classifier. The Random Forest model (0.7679) has a higher accuracy than the Decision Tree model (0.7250). This means that Random Forest makes correct predictions more often overall. Random Forest (0.6128) shows  higher precision compared to Decision Tree (0.4439). Precision is important for minimizing False Positives. Higher precision means that when Random Forest predicts someone is TSA eligible, it's more likely to be correct. Random Forest (0.3956) also has slightly higher recall than Decision Tree (0.3542). Recall is important for minimizing False Negatives. A higher recall means Random Forest is better at identifying those who are actually TSA eligible. This reduces the chance of eligible individuals being missed. Random Forest has fewer False Positives (4393 vs. 7461), supporting its higher precision. Random Forest also has fewer False Negatives (12285 vs. 13009) and more True Positives (7963 vs. 7239), supporting its higher recall. Random Forest correctly identifies a substantially larger number of True Negatives (50359 vs. 47291), meaning better performance in identifying non-eligible individuals.
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import urllib.request
import time
import os

st.title("FEMA Disaster Relief Dashboard")

# URL to the dataset
url = "https://storage.googleapis.com/info_450/IndividualAssistanceHousingRegistrantsLargeDisasters%20(1).csv"
filename = "fema_disaster_data.csv"

# Download CSV only if it doesn't exist locally
if not os.path.exists(filename):
    start = time.time()
    st.info(f"Downloading dataset ({filename})...")
    urllib.request.urlretrieve(url, filename)
    st.success(f"Downloaded {filename} in {time.time() - start:.2f} seconds!")

# Load the dataset
df = pd.read_csv(filename, nrows=300000)

# --- Data preview ---
st.subheader("Data Preview")
st.write(df.head())

# --- Histogram of Repair Amount ---
st.subheader("Histogram of Repair Amount")
fig_hist = px.histogram(
    df,
    x="repairAmount",
    nbins=30,
    title="Distribution of Repair Amounts"
)
st.plotly_chart(fig_hist)

# --- Boxplot of Repair Amount by TSA Eligibility ---
st.subheader("Boxplot: Repair Amount by TSA Eligibility")
fig_box = px.box(
    df,
    x="tsaEligible",
    y="repairAmount",
    title="Repair Amount by TSA Eligibility",
    labels={"tsaEligible": "TSA Eligible (1=Yes, 0=No)",
            "repairAmount": "Repair Amount"}
)
st.plotly_chart(fig_box)

# --- Insight ---
st.markdown(
    "**Insight:** TSA-eligible households tend to show different repair cost patterns. "
    "Compare the spread and median values between the groups."
)
